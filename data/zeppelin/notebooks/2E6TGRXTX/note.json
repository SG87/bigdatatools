{
  "paragraphs": [
    {
      "text": "%md\n\n.\u003ccenter\u003e![ML Logo](http://spark-mooc.github.io/web-assets/images/CS190.1x_Banner_300.png)\n\n![Streaming Logo](https://gcn.com/articles/2014/01/07/~/media/GIG/GCN/Redesign/Articles/2014/January/datastream.png) \u003c/center\u003e\n\n# Spark Streaming\n\n\nThis lab covers a basic introduction to Spark Streaming. It show the fundamental concepts of Spark Streaming transformations and the connection with Kafka. This will be done by connecting the Spark Streaming app to the **IESEG_MBD** topic and doing some basic transformations and actions on the data stream.\n\n## This lab will cover:\n\n* Part 0: Load modules\n* Part 1: Define helper functions\n* Part 2: Specifying arguments and parameters\n* Part 3: Define helper table\n* Part 4: Initialize the StreamingContext()\n* Part 5: Initializing of the stream and creation of the  execution plan\n* Part 6: Start the stream\n* Part 7: Stop the stream\n\n\n\u003e Note that, for reference, you can look up the details of the relevant Spark Streaming methods in [Spark\u0027s Streaming API](https://spark.apache.org/docs/latest/streaming-programming-guide.html)",
      "user": "anonymous",
      "dateUpdated": "2019-04-13 08:27:06.556",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003e.\u003ccenter\u003e\u003cimg src\u003d\"http://spark-mooc.github.io/web-assets/images/CS190.1x_Banner_300.png\" alt\u003d\"ML Logo\" /\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://gcn.com/articles/2014/01/07/~/media/GIG/GCN/Redesign/Articles/2014/January/datastream.png\" alt\u003d\"Streaming Logo\" /\u003e \u003c/center\u003e\u003c/p\u003e\n\u003ch1\u003eSpark Streaming\u003c/h1\u003e\n\u003cp\u003eThis lab covers a basic introduction to Spark Streaming. It show the fundamental concepts of Spark Streaming transformations and the connection with Kafka. This will be done by connecting the Spark Streaming app to the \u003cstrong\u003eIESEG_MBD\u003c/strong\u003e topic and doing some basic transformations and actions on the data stream.\u003c/p\u003e\n\u003ch2\u003eThis lab will cover:\u003c/h2\u003e\n\u003cul\u003e\n  \u003cli\u003ePart 0: Load modules\u003c/li\u003e\n  \u003cli\u003ePart 1: Define helper functions\u003c/li\u003e\n  \u003cli\u003ePart 2: Specifying arguments and parameters\u003c/li\u003e\n  \u003cli\u003ePart 3: Define helper table\u003c/li\u003e\n  \u003cli\u003ePart 4: Initialize the StreamingContext()\u003c/li\u003e\n  \u003cli\u003ePart 5: Initializing of the stream and creation of the execution plan\u003c/li\u003e\n  \u003cli\u003ePart 6: Start the stream\u003c/li\u003e\n  \u003cli\u003ePart 7: Stop the stream\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n  \u003cp\u003eNote that, for reference, you can look up the details of the relevant Spark Streaming methods in \u003ca href\u003d\"https://spark.apache.org/docs/latest/streaming-programming-guide.html\"\u003eSpark\u0026rsquo;s Streaming API\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1553956502125_-38292384",
      "id": "20171101-121445_1929523416",
      "dateCreated": "2019-03-30 14:35:02.125",
      "dateStarted": "2019-04-13 08:27:06.829",
      "dateFinished": "2019-04-13 08:27:11.127",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Part 0: Load Modules\n\nPersonally, I prefer to load all the modules we need in a notebook before starting.",
      "user": "anonymous",
      "dateUpdated": "2019-04-13 08:27:11.151",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePart 0: Load Modules\u003c/h2\u003e\n\u003cp\u003ePersonally, I prefer to load all the modules we need in a notebook before starting.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1553956502129_-2125405992",
      "id": "20171101-122102_957410243",
      "dateCreated": "2019-03-30 14:35:02.129",
      "dateStarted": "2019-04-13 08:27:11.600",
      "dateFinished": "2019-04-13 08:27:11.616",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%dep z.load(\"/conf/spark-streaming-kafka-0-8_2.11-2.2.1.jar\")",
      "user": "anonymous",
      "dateUpdated": "2019-04-13 08:27:11.700",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res0: org.apache.zeppelin.dep.Dependency \u003d org.apache.zeppelin.dep.Dependency@48901e5\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1554648154824_1984088425",
      "id": "20190407-144234_695605242",
      "dateCreated": "2019-04-07 14:42:34.825",
      "dateStarted": "2019-04-13 08:27:12.052",
      "dateFinished": "2019-04-13 08:27:22.988",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nimport sys\nimport json\n\nfrom pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\nfrom pyspark.streaming.kafka import KafkaUtils\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import col",
      "user": "anonymous",
      "dateUpdated": "2019-04-13 08:27:23.078",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1553956502129_90034229",
      "id": "20171030-183117_1217424452",
      "dateCreated": "2019-03-30 14:35:02.129",
      "dateStarted": "2019-04-13 08:27:23.399",
      "dateFinished": "2019-04-13 08:27:37.755",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Part1: Define helper functions\n\nBefore running a streaming application it is smart to create some helper functions.\n\nTo be able to output some results, some things need to be done for each micro-batch of tweets (RDDs).\n\nKafka will create a stream of tweets. These tweets are of type string (containing a json structure). The following steps need to be done for each tweet:\n* Transform the string to a dictionary (key-value structure in Python). To do this, we can use the predefined `json.loads()` function in Python.\n* When we have a dictionary, we need to specify which fields (keys) we want to retain. To do this, we define a `selectFields()` function in Python.\n* After selecting the fields, we want to transform the RDD to a dataframe. To do this, we can use the predefined `toDF()` transformation. Unfortunately Spark will not be able to figure out the type of each field. Therefore we need to specify a schema for the `toDF()` transfromation. Therfore, we are also going to specify a `makeSchema()` function in Python.\n* The `toDF()` transformation will not work directly on the RDD, so we need to include it in a `foreachRDD()` transformation. In PySpark this `foreachRDD()` transformation can contain all futher transformation and actions the are going to be defined on the dataframe. In this case the helper functions `DFActions()` is defined and contains the following transactions and actions:\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;- `toDF()` to transform the RDD to DF\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;- `filter()` to remove tweets without _id_\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;- `show()` to show some sample tweets\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;- `count()`to count the number of tweet during the past 10 seconds",
      "user": "anonymous",
      "dateUpdated": "2019-04-13 08:27:37.840",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePart1: Define helper functions\u003c/h2\u003e\n\u003cp\u003eBefore running a streaming application it is smart to create some helper functions.\u003c/p\u003e\n\u003cp\u003eTo be able to output some results, some things need to be done for each micro-batch of tweets (RDDs).\u003c/p\u003e\n\u003cp\u003eKafka will create a stream of tweets. These tweets are of type string (containing a json structure). The following steps need to be done for each tweet:\u003cbr/\u003e* Transform the string to a dictionary (key-value structure in Python). To do this, we can use the predefined \u003ccode\u003ejson.loads()\u003c/code\u003e function in Python.\u003cbr/\u003e* When we have a dictionary, we need to specify which fields (keys) we want to retain. To do this, we define a \u003ccode\u003eselectFields()\u003c/code\u003e function in Python.\u003cbr/\u003e* After selecting the fields, we want to transform the RDD to a dataframe. To do this, we can use the predefined \u003ccode\u003etoDF()\u003c/code\u003e transformation. Unfortunately Spark will not be able to figure out the type of each field. Therefore we need to specify a schema for the \u003ccode\u003etoDF()\u003c/code\u003e transfromation. Therfore, we are also going to specify a \u003ccode\u003emakeSchema()\u003c/code\u003e function in Python.\u003cbr/\u003e* The \u003ccode\u003etoDF()\u003c/code\u003e transformation will not work directly on the RDD, so we need to include it in a \u003ccode\u003eforeachRDD()\u003c/code\u003e transformation. In PySpark this \u003ccode\u003eforeachRDD()\u003c/code\u003e transformation can contain all futher transformation and actions the are going to be defined on the dataframe. In this case the helper functions \u003ccode\u003eDFActions()\u003c/code\u003e is defined and contains the following transactions and actions:\u003cbr/\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;- \u003ccode\u003etoDF()\u003c/code\u003e to transform the RDD to DF\u003cbr/\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;- \u003ccode\u003efilter()\u003c/code\u003e to remove tweets without \u003cem\u003eid\u003c/em\u003e\u003cbr/\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;- \u003ccode\u003eshow()\u003c/code\u003e to show some sample tweets\u003cbr/\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;- \u003ccode\u003ecount()\u003c/code\u003eto count the number of tweet during the past 10 seconds\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1553956502130_909924782",
      "id": "20171101-122115_1457601757",
      "dateCreated": "2019-03-30 14:35:02.130",
      "dateStarted": "2019-04-13 08:27:37.925",
      "dateFinished": "2019-04-13 08:27:37.969",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ndef selectFields(tweet, fields):    \n\t\"\"\" Function that translates a tweet to a list of selected values:    \n\targs:        \n\t    tweet \u003d a tweet (string containing json structure)        \n\t    fiels \u003d list of tuples, where each tuple contains two strings (string, string). The tuples contains (\"field_name\", \"field_type\"). If we need a nested field_name, use \".\" to separate parent and child (remark: here we can have at most 5 nested levels.)    \n\treturn:        \n\t    list of values of the selected field_name keys    \n\t\"\"\"            \n\tl \u003d list()    \n\tfor field in fields:        \n\t\ttry:            \n\t\t\tif \".\" in field[0]:                \n\t\t\t\tf \u003d field[0].split(\".\")\n\t\t\t\tif len(f) \u003d\u003d 2:                    \n\t\t\t\t\td \u003d tweet[f[0]][f[1]]                \n\t\t\t\telif len(f) \u003d\u003d 3:                    \n\t\t\t\t\td \u003d tweet[f[0]][f[1]][f[2]]                \n\t\t\t\telif len(f) \u003d\u003d 4:                    \n\t\t\t\t\td \u003d tweet[f[0]][f[1]][f[2]][f[3]]               \n\t\t\t\telif len(f) \u003d\u003d 5:                    \n\t\t\t\t\td \u003d tweet[f[0]][f[1]][f[2]][f[3]][f[4]]           \n\t\t\telse:                \n\t\t\t\td \u003d tweet[field[0]]        \n\t\texcept:            \n\t\t\td \u003d None        \n\t\tl.append(d)    \n\treturn l\n\n    \n    \ndef makeSchema(fields):\n    \"\"\" Function that creates a schema based on a list of fields:\n    args:\n        fiels \u003d list of tuples, where each tuple contains two strings (string, string). The tuples contains (\"field_name\", \"field_type\")\n    return:\n        StructType object containing the schema to transfer RDD to DF using toDF() function\n    \"\"\"\n    schema\u003dlist()\n    for field in fields:\n        if field[1] \u003d\u003d \"int\":\n            s \u003d StructField(field[0], IntegerType(), True)\n        if field[1] \u003d\u003d \"long\":\n            s \u003d StructField(field[0], LongType(), True)\n        elif field[1] \u003d\u003d \"date\":\n            s \u003d StructField(field[0], DateType(), True)\n        elif field[1] \u003d\u003d \"str\":\n            s \u003d StructField(field[0], StringType(), True)\n        elif field[1] \u003d\u003d \"float\":\n            s \u003d StructField(field[0], FloatType(), True)\n        schema.append(s)\n    return StructType(schema)\n    \n    \ndef DFActions(rdd):\n    \"\"\" Function that defines the transactions and actions to perform on the DStream Dataframe:\n    args:\n        rdd \u003d the last RDD before tranforming to DataFrame\n    \"\"\"\n    DF \u003d rdd.toDF(schema).filter(col(\"id\").isNotNull())\n    DF \u003d DF.join(languagesDF, \"lang\", \"left\")\n    DF.show(5)\n    print(\"Number of tweet last \" + str(seconds_to_run) + \" seconds: \" + str(DF.count()))\n    \n    ",
      "user": "anonymous",
      "dateUpdated": "2019-04-13 08:27:38.024",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1553956502130_1347194912",
      "id": "20171030-205833_757769488",
      "dateCreated": "2019-03-30 14:35:02.130",
      "dateStarted": "2019-04-13 08:27:38.134",
      "dateFinished": "2019-04-13 08:27:38.241",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Part 2: Specifying arguments and parameters\n\nThe helper functions defined above take arguments as input. In the cell below, the **fields** argument for the **selectFields** and **makeSchema** function is defined. Additionally we define to which topic Spark should listen, the interval Spark Streaming should run and finally the location of Zookeeper.",
      "user": "anonymous",
      "dateUpdated": "2019-04-13 08:27:38.335",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePart 2: Specifying arguments and parameters\u003c/h2\u003e\n\u003cp\u003eThe helper functions defined above take arguments as input. In the cell below, the \u003cstrong\u003efields\u003c/strong\u003e argument for the \u003cstrong\u003eselectFields\u003c/strong\u003e and \u003cstrong\u003emakeSchema\u003c/strong\u003e function is defined. Additionally we define to which topic Spark should listen, the interval Spark Streaming should run and finally the location of Zookeeper.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1553956502131_-1965875866",
      "id": "20171101-124219_11977135",
      "dateCreated": "2019-03-30 14:35:02.131",
      "dateStarted": "2019-04-13 08:27:38.389",
      "dateFinished": "2019-04-13 08:27:38.404",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfields \u003d [(\"id\", \"long\"), (\"created_at\", \"str\"), (\"text\", \"str\"), (\"user.id\", \"long\"), (\"user.name\", \"str\"), (\"user.location\", \"str\"), (\"user.followers_count\", \"int\"), (\"user.friends_count\", \"int\"), (\"entities.hashtags\", \"str\"), (\"entities.user_mentions\", \"str\"), (\"lang\", \"str\"), (\"timestamp_ms\", \"str\")]\n\n#Topic to listen to \ntopic \u003d \"trump\"\n\n#Interval\nseconds_to_run \u003d 10\n\n# Location of Zookeeper\nzkQuorum \u003d \"zookeeper:2181\"",
      "user": "anonymous",
      "dateUpdated": "2019-04-13 08:27:38.489",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1553956502131_-1758035865",
      "id": "20171030-203558_547634945",
      "dateCreated": "2019-03-30 14:35:02.131",
      "dateStarted": "2019-04-13 08:27:38.558",
      "dateFinished": "2019-04-13 08:27:38.631",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Part 3: Define helper table\nThis part serves merely as example that shows the possiblity to merge a stream with a normal Dataframe. Let us for example create a DF that contains languages and the abbreviation of languages. The Tweets contain a veriable _lang_ containing the abbreviation of the language of the tweet. Te dataframe we created can be joined with the stream to enrich it with the full version of the language. Notice that we `cache()` the DF, because it will be reused in every iteration of the stream.",
      "user": "anonymous",
      "dateUpdated": "2019-04-13 08:27:38.659",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePart 3: Define helper table\u003c/h2\u003e\n\u003cp\u003eThis part serves merely as example that shows the possiblity to merge a stream with a normal Dataframe. Let us for example create a DF that contains languages and the abbreviation of languages. The Tweets contain a veriable \u003cem\u003elang\u003c/em\u003e containing the abbreviation of the language of the tweet. Te dataframe we created can be joined with the stream to enrich it with the full version of the language. Notice that we \u003ccode\u003ecache()\u003c/code\u003e the DF, because it will be reused in every iteration of the stream.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1553956502131_-1783775160",
      "id": "20171101-184132_1418378798",
      "dateCreated": "2019-03-30 14:35:02.131",
      "dateStarted": "2019-04-13 08:27:38.737",
      "dateFinished": "2019-04-13 08:27:38.755",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nlang_list \u003d  [(\"en\", \"English\"), (\"hi\", \"Hindi\"), (\"fr\", \"French\"), (\"de\", \"German\"), (\"nl\", \"Dutch\"), (\"ar\", \"arabic\"), (\"ja\", \"Japanese\"), (\"ru\", \"Russian\"), (\"es\", \"Spanish\"), (\"zh\", \"Chinese\")]\n\nlanguagesDF \u003d spark.createDataFrame(lang_list, [\"lang\", \"language\"]).cache()\nlanguagesDF.show()",
      "user": "anonymous",
      "dateUpdated": "2019-04-13 08:27:38.838",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+----+--------+\n|lang|language|\n+----+--------+\n|  en| English|\n|  hi|   Hindi|\n|  fr|  French|\n|  de|  German|\n|  nl|   Dutch|\n|  ar|  arabic|\n|  ja|Japanese|\n|  ru| Russian|\n|  es| Spanish|\n|  zh| Chinese|\n+----+--------+\n\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://172.21.0.6:4040/jobs/job?id\u003d0"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1553956502132_6894310",
      "id": "20171101-185423_1429285576",
      "dateCreated": "2019-03-30 14:35:02.132",
      "dateStarted": "2019-04-13 08:27:38.905",
      "dateFinished": "2019-04-13 08:27:44.592",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Part 4: Initialize the StreamingContext (ssc)\n\nThe most import aspect of the Spark Streaming is the `StreamingContext()`. This context is build on top oft the `SparkContext()` (sc) and is defined by setting the SparkContext and interval to run.",
      "user": "anonymous",
      "dateUpdated": "2019-04-13 08:27:44.599",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePart 4: Initialize the StreamingContext (ssc)\u003c/h2\u003e\n\u003cp\u003eThe most import aspect of the Spark Streaming is the \u003ccode\u003eStreamingContext()\u003c/code\u003e. This context is build on top oft the \u003ccode\u003eSparkContext()\u003c/code\u003e (sc) and is defined by setting the SparkContext and interval to run.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1553956502132_-933283560",
      "id": "20171101-124235_73838084",
      "dateCreated": "2019-03-30 14:35:02.132",
      "dateStarted": "2019-04-13 08:27:44.727",
      "dateFinished": "2019-04-13 08:27:44.747",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nssc \u003d StreamingContext(sc, seconds_to_run)",
      "user": "anonymous",
      "dateUpdated": "2019-04-13 08:27:44.826",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1553956502132_476131453",
      "id": "20171030-183328_1088012018",
      "dateCreated": "2019-03-30 14:35:02.132",
      "dateStarted": "2019-04-13 08:27:44.905",
      "dateFinished": "2019-04-13 08:27:45.074",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Part 5: Initializing of the stream and creation of the  execution plan\n\nIn the cell below, the `KafkaUtils.createStream()` method is used to create a stream. The argument given are the StreamingContext (ssc), the ZooKeeper location (which we specified above), the name of the stream application, a dict with topics to listen to (which we specified above) as key and the number of partitions as value. To see all possible arguments, type `help(KafkaUtils.createStream)`\n\nAfter the initialization of the stream, the transformations and actions to be executed on the stream are defined.",
      "user": "anonymous",
      "dateUpdated": "2019-04-13 08:27:45.107",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePart 5: Initializing of the stream and creation of the execution plan\u003c/h2\u003e\n\u003cp\u003eIn the cell below, the \u003ccode\u003eKafkaUtils.createStream()\u003c/code\u003e method is used to create a stream. The argument given are the StreamingContext (ssc), the ZooKeeper location (which we specified above), the name of the stream application, a dict with topics to listen to (which we specified above) as key and the number of partitions as value. To see all possible arguments, type \u003ccode\u003ehelp(KafkaUtils.createStream)\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eAfter the initialization of the stream, the transformations and actions to be executed on the stream are defined.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1553956502133_-783274503",
      "id": "20171101-185843_1737487595",
      "dateCreated": "2019-03-30 14:35:02.133",
      "dateStarted": "2019-04-13 08:27:45.192",
      "dateFinished": "2019-04-13 08:27:45.221",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# initialization of the Stream\nprint(zkQuorum)\ntweets \u003d KafkaUtils.createStream(ssc, zkQuorum, \"spark-streaming-consumer\", {topic: 1})\n\n# string to directory\ntweet \u003d tweets.map(lambda a: json.loads(a[1]))\n\n# select the necessary fields\ntweet_selected \u003d tweet.map(lambda c: selectFields(c, fields))\n\n# define schema\nschema \u003d makeSchema(fields)\n\n# transform to DF and perform the DFActions\ntweet_selected.foreachRDD(toDF)",
      "user": "anonymous",
      "dateUpdated": "2019-04-13 08:27:45.291",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "ERROR:root:Exception while sending command.\nTraceback (most recent call last):\n  File \"/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n    response \u003d connection.send_command(command)\n  File \"/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1040, in send_command\n    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\nPy4JNetworkError: Error while receiving\nzookeeper:2181\n\u001b[0;31m\u001b[0m\n\u001b[0;31mPy4JError\u001b[0mTraceback (most recent call last)\n\u001b[0;32m\u003cipython-input-14-0f58a98ca102\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initialization of the Stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzkQuorum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mKafkaUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzkQuorum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"spark-streaming-consumer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# string to directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/streaming/kafka.py\u001b[0m in \u001b[0;36mcreateStream\u001b[0;34m(ssc, zkQuorum, groupId, topics, kafkaParams, storageLevel, keyDecoder, valueDecoder)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mjlevel\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getJavaStorageLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorageLevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mhelper\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mKafkaUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 70\u001b[0;31m         \u001b[0mjstream\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkafkaParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mser\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mPairDeserializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNoOpSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoOpSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mDStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value \u003d get_return_value(\n\u001b[0;32m-\u003e 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             raise Py4JError(\n\u001b[1;32m    326\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 327\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o69.createStream"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1553956502133_39760545",
      "id": "20171030-183514_638708508",
      "dateCreated": "2019-03-30 14:35:02.133",
      "dateStarted": "2019-04-13 08:27:45.373",
      "dateFinished": "2019-04-13 08:27:45.614",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Part 6: Start the stream\nIn this part we start the stream Nevertheless the cell below has only two lines, this is were the magic happens.\n\nAdditionally, the `awaitTermination()`method is called on the stream. This means that the SparkContext waits untill all the transformations and actions are execute before starting a new micro-batch.\n",
      "user": "anonymous",
      "dateUpdated": "2019-03-30 14:35:02.134",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePart 6: Start the stream\u003c/h2\u003e\n\u003cp\u003eIn this part we start the stream Nevertheless the cell below has only two lines, this is were the magic happens.\u003c/p\u003e\n\u003cp\u003eAdditionally, the \u003ccode\u003eawaitTermination()\u003c/code\u003emethod is called on the stream. This means that the SparkContext waits untill all the transformations and actions are execute before starting a new micro-batch.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1553956502133_-587519754",
      "id": "20171101-191017_1555597499",
      "dateCreated": "2019-03-30 14:35:02.133",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nssc.start()\nssc.awaitTermination()",
      "user": "anonymous",
      "dateUpdated": "2019-03-30 14:35:02.134",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n+----+------------------+--------------------+--------------------+----------+------------+-------------+--------------------+------------------+--------------------+----------------------+-------------+--------+\n|lang|                id|          created_at|                text|   user.id|   user.name|user.location|user.followers_count|user.friends_count|   entities.hashtags|entities.user_mentions| timestamp_ms|language|\n+----+------------------+--------------------+--------------------+----------+------------+-------------+--------------------+------------------+--------------------+----------------------+-------------+--------+\n|  en|927821009746612224|Tue Nov 07 08:52:...|IESEG_MBD is awes...|2754263935|Stijn Geuens|         null|                  29|                59|[{indices\u003d[22, 31...|                    []|1510044741461| English|\n+----+------------------+--------------------+--------------------+----------+------------+-------------+--------------------+------------------+--------------------+----------------------+-------------+--------+\n\nNumber of tweet last 10 seconds: 1\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n|lang| id|created_at|text|user.id|user.name|user.location|user.followers_count|user.friends_count|entities.hashtags|entities.user_mentions|timestamp_ms|language|\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n+----+---+----------+----+-------+---------+-------------+--------------------+------------------+-----------------+----------------------+------------+--------+\n\nNumber of tweet last 10 seconds: 0\n"
          },
          {
            "type": "TEXT",
            "data": "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-3650523266338870301.py\", line 367, in \u003cmodule\u003e\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-3650523266338870301.py\", line 360, in \u003cmodule\u003e\n    exec(code, _zcUserQueryNameSpace)\n  File \"\u003cstdin\u003e\", line 2, in \u003cmodule\u003e\n  File \"/usr/share/spark/python/pyspark/streaming/context.py\", line 206, in awaitTermination\n    self._jssc.awaitTermination()\n  File \"/usr/share/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n    answer \u003d self.gateway_client.send_command(command)\n  File \"/usr/share/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n    response \u003d connection.send_command(command)\n  File \"/usr/share/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1028, in send_command\n    answer \u003d smart_decode(self.stream.readline()[:-1])\n  File \"/usr/lib/python2.7/socket.py\", line 451, in readline\n    data \u003d self._sock.recv(self._rbufsize)\n  File \"/usr/share/spark/python/pyspark/context.py\", line 237, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1553956502134_810147520",
      "id": "20171030-183553_1289449604",
      "dateCreated": "2019-03-30 14:35:02.134",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Part 7: Stop the streal",
      "user": "anonymous",
      "dateUpdated": "2019-03-30 14:35:02.134",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePart 7: Stop the streal\u003c/h2\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1553956502134_105721705",
      "id": "20171101-191234_793401199",
      "dateCreated": "2019-03-30 14:35:02.134",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nssc.stop()",
      "user": "anonymous",
      "dateUpdated": "2019-03-30 14:35:02.135",
      "config": {
        "editorSetting": {
          "language": "python"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1553956502135_290155730",
      "id": "20171030-184122_1370937705",
      "dateCreated": "2019-03-30 14:35:02.135",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2019-03-30 14:35:02.135",
      "config": {
        "editorSetting": {
          "language": "python"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1553956502135_1475980950",
      "id": "20171030-184443_171016630",
      "dateCreated": "2019-03-30 14:35:02.135",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Spark Streaming/IESEG_MBD",
  "id": "2E6TGRXTX",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}