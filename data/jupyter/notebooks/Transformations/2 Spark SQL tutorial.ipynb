{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)  ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "# **Spark Tutorial: Learning Apache Spark**\n",
    "#### This tutorial will teach you how to use [Apache Spark](http://spark.apache.org/), a framework for large-scale data processing, within a notebook. Many traditional frameworks were designed to be run on a single computer.  However, many datasets today are too large to be stored on a single computer, and even when a dataset can be stored on one computer (such as the datasets in this tutorial), the dataset can often be processed much more quickly using multiple computers.  Spark has efficient implementations of a number of transformations and actions that can be composed together to perform data processing and analysis.  Spark excels at distributing these operations across a cluster while abstracting away many of the underlying implementation details.  Spark has been designed with a focus on scalability and efficiency.  *With Spark you can begin developing your solution on your laptop, using a small dataset, and then use that same code to process terabytes or even petabytes across a distributed cluster.*\n",
    "#### **During this tutorial we will cover:**\n",
    "#### * *Part 1:* Basic notebook usage and [Python](https://docs.python.org/2/) integration\n",
    "#### * *Part 2:* An introduction to using [Apache Spark](https://spark.apache.org/) with the [PySpark SQL API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module) running in a notebook\n",
    "#### * *Part 3:* Using DataFrames and chaining together transformations and actions\n",
    "#### * *Part 4*: Python Lambda functions and User Defined Functions\n",
    "#### * *Part 5:* Additional DataFrame actions\n",
    "#### * *Part 6:* Additional DataFrame transformations\n",
    "#### * *Part 7:* Unioning and joining DataFrames\n",
    "#### * *Part 8:* Caching DataFrames and storage options\n",
    "\n",
    "\n",
    "###  The following methods will be covered:\n",
    "* `createDataFrame()`, `read()`, `write()`\n",
    "\n",
    "####  The following transformations will be covered:\n",
    "* `select()`, `filter()`, `distinct()`, `dropDuplicates()`, `orderBy()`, `groupBy()`, `withColumn()`, `withColumnRenamed()`, `sample()`, `union()`, `join()`\n",
    "\n",
    "###  The following actions will be covered:\n",
    "* `first()`, `take()`, `count()`, `collect()`, `show()`\n",
    "\n",
    "####  Also covered:\n",
    "* `cache()`, `unpersist()`\n",
    "\n",
    "#### Note that, for reference, you can look up the details of these methods in the [Spark's PySpark SQL API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1: Basic notebook usage and [Python](https://docs.python.org/2/) integration **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1a) Notebook usage**\n",
    "#### A notebook is comprised of a linear sequence of cells.  These cells can contain either markdown or code, but we won't mix both in one cell.  When a markdown cell is executed it renders formatted text, images, and links just like HTML in a normal webpage.  The text you are reading right now is part of a markdown cell.  Python code cells allow you to execute arbitrary Python commands just like in any Python shell. Place your cursor inside the cell below, and press \"Shift\" + \"Enter\" to execute the code and advance to the next cell.  You can also press \"Ctrl\" + \"Enter\" to execute the code and remain in the cell.  These commands work the same in both markdown and code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of 1 and 1 is 2\n"
     ]
    }
   ],
   "source": [
    "# This is a Python cell. You can run normal Python code here...\n",
    "print('The sum of 1 and 1 is ' + str(1+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of x is 42\n"
     ]
    }
   ],
   "source": [
    "# Here is another Python cell, this time with a variable (x) declaration and an if statement:\n",
    "x = 42\n",
    "if x > 40:\n",
    "    print('The value of x is ' + str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1b) Notebook state**\n",
    "#### As you work through a notebook it is important that you run all of the code cells.  The notebook is stateful, which means that variables and their values are retained until the kernel is restarted.  If you do not run all of the code cells as you proceed through the notebook, your variables will not be properly initialized and later code might fail.  You will also need to rerun any cells that you have modified in order for the changes to be available to other cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "# This cell relies on x being defined already.\n",
    "# If we didn't run the cells from part (1a) this code would fail.\n",
    "print(x * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1c) Library imports**\n",
    "#### We can import standard Python libraries ([modules](https://docs.python.org/2/tutorial/modules.html)) the usual way.  An `import` statement will import the specified module.  In this tutorial and future labs, we will provide any imports that are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the regular expression library\n",
    "import re\n",
    "m = re.search('(?<=abc)def', 'abcdef')\n",
    "m.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was last run on: 2019-05-02 06:53:02.196911\n"
     ]
    }
   ],
   "source": [
    "# Import the datetime library\n",
    "import datetime\n",
    "print('This was last run on: ' + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2: An introduction to using [Apache Spark](https://spark.apache.org/) with the [PySpark SQL API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module) running in a notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Context\n",
    "\n",
    "#### In Spark, communication occurs between a driver and executors.  The driver has Spark jobs that it needs to run and these jobs are split into tasks that are submitted to the executors for completion.  The results from these tasks are delivered back to the driver.\n",
    "\n",
    "#### In part 1, we saw that normal Python code can be executed via cells. When using  Jupyter notebook it is executed within the kernel associated with the notebook. Since no Spark functionality is actually being used, no tasks are launched on the executors.\n",
    "\n",
    "#### In order to use Spark and its DataFrame API we will need to use a `SQLContext`.  When running Spark, you start a new Spark application by creating a [SparkContext](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext). You can then create a [SQLContext](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext) from the `SparkContext`. When the `SparkContext` is created, it asks the master for some cores to use to do work.  The master sets these cores aside just for you; they won't be used for other applications. When using a notebook, both a `SparkContext` and a `SQLContext` are created for you automatically. `sc` is your `SparkContext`, and `sqlContext` is your `SQLContext`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting from Spark 2.0, the seperate creation of your `SparkContext` and `SQLContext` is not necessary anymore. By creating a [`SparkSession`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession.newSession), both are automatically created within the SparkSession. The SparkSession is a new session, that has separate SQLConf, registered temporary views and UDFs, but shared SparkContext and table cache.\n",
    "\n",
    "#### In Notebooks (Jupyter, Zeppelin, etc.) and CLIs (pyspark, spark-shell, etc.) the contexts and sessions are automatically created at the start of the session. \n",
    "\n",
    "#### If you run an application the following lines of code should be included to create the contexts and sessions:\n",
    "\n",
    "##### # Import necessary pyspark and pyspark.sql packages\n",
    "from pyspark import SparkContext \n",
    "<br /> \n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "##### # create Spark Context as sc\n",
    "sc = SparkContext(appName = \"First steps in Spark\")\n",
    "\n",
    "##### # create SQL Context as sc\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "##### Create spark session\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"First steps in Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) Example Cluster\n",
    "#### The diagram shows an example cluster, where the slots allocated for an application are outlined in purple. (Note: We're using the term _slots_ here to indicate threads available to perform parallel work for Spark.) Spark documentation often refers to these threads as _cores_, which is a confusing term, as the number of slots available on a particular machine does not necessarily have any relationship to the number of physical CPU cores on that machine.)\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-2a.png\" style=\"height: 800px\"/>\n",
    " \n",
    "####  You can view the details of your Spark application in the Spark web UI on *IP*:4040.  In the web UI, under the \"Jobs\" tab, you can see a list of jobs that have been scheduled or run.  It's likely there isn't any thing interesting here yet because we haven't run any jobs, but we'll return to this page later. \n",
    "####  At a high level, every Spark application consists of a driver program that launches various parallel operations on executor Java Virtual Machines (JVMs) running either in a cluster or locally on the same machine.  When running locally, `pyspark` is the driver program. In all cases, this driver program contains the main loop for the program and creates distributed datasets on the cluster, then applies operations (transformations & actions) to those datasets.\n",
    "####  Driver programs access Spark through a SparkContext object, which represents a connection to a computing cluster. A Spark SQL context object (`sqlContext`) is the main entry point for Spark DataFrame and SQL functionality. A `SQLContext` can be used to create DataFrames, which allows you to direct the operations on your data.\n",
    " \n",
    "####  Try printing out `sqlContext` to see its type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.context.SQLContext"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the type of the Spark sqlContext\n",
    "type(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### that the type is `HiveContext`. This means we're working with a version of Spark that has Hive support. Compiling Spark with Hive support is a good idea, even if you don't have a Hive metastore. As the\n",
    "#### [Spark Programming Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#starting-point-sqlcontext) states, a `HiveContext` \"provides a superset of the functionality provided by the basic `SQLContext`. Additional features include the ability to write queries using the more complete HiveQL parser, access to Hive UDFs [user-defined functions], and the ability to read data from Hive tables. To use a `HiveContext`, you do not need to have an existing Hive setup, and all of the data sources available to a `SQLContext` are still available.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) SparkContext attributes\n",
    "####  You can use Python's [dir()](https://docs.python.org/2/library/functions.html?highlight=dir#dir) function to get a list of all the attributes (including methods) accessible through the `sqlContext` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_conf',\n",
       " '_inferSchema',\n",
       " '_instantiatedContext',\n",
       " '_jsc',\n",
       " '_jsqlContext',\n",
       " '_jvm',\n",
       " '_sc',\n",
       " '_ssql_ctx',\n",
       " 'cacheTable',\n",
       " 'clearCache',\n",
       " 'createDataFrame',\n",
       " 'createExternalTable',\n",
       " 'dropTempTable',\n",
       " 'getConf',\n",
       " 'getOrCreate',\n",
       " 'newSession',\n",
       " 'range',\n",
       " 'read',\n",
       " 'readStream',\n",
       " 'registerDataFrameAsTable',\n",
       " 'registerFunction',\n",
       " 'registerJavaFunction',\n",
       " 'setConf',\n",
       " 'sparkSession',\n",
       " 'sql',\n",
       " 'streams',\n",
       " 'table',\n",
       " 'tableNames',\n",
       " 'tables',\n",
       " 'udf',\n",
       " 'uncacheTable']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outside of `pyspark` or a notebook, `SQLContext` is created from the lower-level `SparkContext`, which is usually used to create Resilient Distributed Datasets (RDDs). An RDD is the way Spark actually represents data internally; DataFrames are actually implemented in terms of RDDs.\n",
    "\n",
    "#### While you can interact directly with RDDs, DataFrames are preferred. They're generally faster, and they perform the same no matter what language (Python, R, Scala or Java) you use with Spark.\n",
    "\n",
    "#### In this course, we'll be using DataFrames, so we won't be interacting directly with the Spark Context object very much. However, it's worth knowing that inside `pyspark` or a notebook, you already have an existing `SparkContext` in the `sc` variable. One simple thing we can do with `sc` is check the version of Spark we're using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After reading the help we've decided we want to use sc.version to see what version of Spark we are running\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://84e319f2341b:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://84e319f2341b:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff95bac34e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class list in module builtins:\n",
      "\n",
      "class list(object)\n",
      " |  list() -> new empty list\n",
      " |  list(iterable) -> new list initialized from iterable's items\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, value, /)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __iadd__(self, value, /)\n",
      " |      Implement self+=value.\n",
      " |  \n",
      " |  __imul__(self, value, /)\n",
      " |      Implement self*=value.\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mul__(self, value, /)\n",
      " |      Return self*value.n\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __reversed__(...)\n",
      " |      L.__reversed__() -- return a reverse iterator over the list\n",
      " |  \n",
      " |  __rmul__(self, value, /)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      L.__sizeof__() -- size of L in memory, in bytes\n",
      " |  \n",
      " |  append(...)\n",
      " |      L.append(object) -> None -- append object to end\n",
      " |  \n",
      " |  clear(...)\n",
      " |      L.clear() -> None -- remove all items from L\n",
      " |  \n",
      " |  copy(...)\n",
      " |      L.copy() -> list -- a shallow copy of L\n",
      " |  \n",
      " |  count(...)\n",
      " |      L.count(value) -> integer -- return number of occurrences of value\n",
      " |  \n",
      " |  extend(...)\n",
      " |      L.extend(iterable) -> None -- extend list by appending elements from the iterable\n",
      " |  \n",
      " |  index(...)\n",
      " |      L.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      " |      Raises ValueError if the value is not present.\n",
      " |  \n",
      " |  insert(...)\n",
      " |      L.insert(index, object) -- insert object before index\n",
      " |  \n",
      " |  pop(...)\n",
      " |      L.pop([index]) -> item -- remove and return item at index (default last).\n",
      " |      Raises IndexError if list is empty or index is out of range.\n",
      " |  \n",
      " |  remove(...)\n",
      " |      L.remove(value) -> None -- remove first occurrence of value.\n",
      " |      Raises ValueError if the value is not present.\n",
      " |  \n",
      " |  reverse(...)\n",
      " |      L.reverse() -- reverse *IN PLACE*\n",
      " |  \n",
      " |  sort(...)\n",
      " |      L.sort(key=None, reverse=False) -> None -- stable sort *IN PLACE*\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Help can be used on any Python object\n",
    "help(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3: Using DataFrames and chaining together transformations and actions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with your first DataFrames\n",
    " \n",
    "#### In Spark, we first create a base [DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame). We can then apply one or more transformations to that base DataFrame. *A DataFrame is immutable, so once it is created, it cannot be changed.* As a result, each transformation creates a new DataFrame. Finally, we can apply one or more actions to the DataFrames.\n",
    "#### > Note that Spark uses lazy evaluation, so transformations are not actually executed until an action occurs.\n",
    "\n",
    "#### We will perform several exercises to obtain a better understanding of DataFrames:\n",
    "#### * Create a Python collection of 10,000 integers\n",
    "#### * Create a Spark DataFrame from that collection\n",
    "#### * Create a Spark Dataframe from a source\n",
    "#### * Subtract one from each value using `map`\n",
    "#### * Perform action `collect` to view results\n",
    "#### * Perform action `count` to view counts\n",
    "#### * Apply transformation `filter` and view results with `collect`\n",
    "#### * Learn about lambda functions\n",
    "#### * Explore how lazy evaluation works and the debugging challenges that it introduces\n",
    "#### * Write to a file\n",
    "\n",
    "#### A DataFrame consists of a series of `Row` objects; each `Row` object has a set of named columns. You can think of a DataFrame as modeling a table, though the data source being processed does not have to be a table.\n",
    " \n",
    "#### More formally, a DataFrame must have a _schema_, which means it must consist of columns, each of which has a _name_ and a _type_. Some data sources have schemas built into them. Examples include RDBMS databases, Parquet files, and NoSQL databases like Cassandra. Other data sources don't have computer-readable schemas, but you can often apply a schema programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3a) Create a Python collection of 10,000 people\n",
    "\n",
    "#### We will use a third-party Python testing library called [fake-factory](https://pypi.python.org/pypi/fake-factory/0.5.3) to create a collection of fake person records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%sh\n",
    "#pip3 install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Factory\n",
    "fake = Factory.create()\n",
    "fake.seed(4321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### We're going to use this factory to create a collection of randomly generated people records. In the next section, we'll turn that collection into a DataFrame. We'll use a Python tuple to help us define the Spark DataFrame schema. There are other ways to define schemas, though; see the Spark Programming Guide's discussion of [schema inference](http://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection) for more information. (For instance,\n",
    "we could also use a Python `namedtuple` or a Spark `Row` object.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each entry consists of last_name, first_name, ssn, job, and age (at least 1)\n",
    "from pyspark.sql import Row\n",
    "def fake_entry():\n",
    "  name = fake.name().split()\n",
    "  return (name[1], name[0], fake.ssn(), fake.job(), abs(2016 - fake.date_time().year) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper function to call a function repeatedly\n",
    "def repeat(times, func, *args, **kwargs):\n",
    "    for _ in range(times):\n",
    "        yield func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(repeat(10000, fake_entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `data` is just a normal Python list, containing Python tuples objects. Let's look at the first item in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Distributed data and using a collection to create a DataFrame\n",
    "\n",
    "#### In Spark, datasets are represented as a list of entries, where the list is broken up into many different partitions that are each stored on a different machine.  Each partition holds a unique subset of the entries in the list.  Spark calls datasets that it stores \"Resilient Distributed Datasets\" (RDDs). Even DataFrames are ultimately represented as RDDs, with additional meta-data.\n",
    " \n",
    "#### <img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3b.png\" style=\"width: 900px; margin: 5px\"/>\n",
    "\n",
    "#### One of the defining features of Spark, compared to other data analytics frameworks (e.g., Hadoop), is that it stores data in memory rather than on disk.  This allows Spark applications to run much more quickly, because they are not slowed down by needing to read data from disk.\n",
    "#### The figure to the right illustrates how Spark breaks a list of data entries into partitions that are each stored in memory on a worker.\n",
    " \n",
    " \n",
    "####  To create the DataFrame, we'll use `spark.createDataFrame()`, and we'll pass our array of data in as an argument to that function. Spark will create a new set of input data based on data that is passed in.  A DataFrame requires a _schema_, which is a list of columns, where each column has a name and a type. Our list of data has elements with types (mostly strings, but one integer). We'll supply the rest of the schema and the column names as the second argument to `createDataFrame()`.\n",
    "\n",
    "#### The [`createDataFrame()` method](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession.createDataFrame) can take an RDD, Python list or Python Pandas DafaFrame as input.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's view the help for `createDataFrame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n",
      "\n",
      "createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True) method of pyspark.sql.session.SparkSession instance\n",
      "    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      "    \n",
      "    When ``schema`` is a list of column names, the type of each column\n",
      "    will be inferred from ``data``.\n",
      "    \n",
      "    When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "    from ``data``, which should be an RDD of :class:`Row`,\n",
      "    or :class:`namedtuple`, or :class:`dict`.\n",
      "    \n",
      "    When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      "    the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      "    :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "    :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\",\n",
      "    each record will also be wrapped into a tuple, which can be converted to row later.\n",
      "    \n",
      "    If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      "    rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      "    \n",
      "    :param data: an RDD of any kind of SQL data representation(e.g. row, tuple, int, boolean,\n",
      "        etc.), or :class:`list`, or :class:`pandas.DataFrame`.\n",
      "    :param schema: a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "        column names, default is ``None``.  The data type string format equals to\n",
      "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "        omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      "        ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`. We can also use\n",
      "        ``int`` as a short name for ``IntegerType``.\n",
      "    :param samplingRatio: the sample ratio of rows used for inferring\n",
      "    :param verifySchema: verify data types of every row against schema.\n",
      "    :return: :class:`DataFrame`\n",
      "    \n",
      "    .. versionchanged:: 2.1\n",
      "       Added verifySchema.\n",
      "    \n",
      "    .. note:: Usage with spark.sql.execution.arrow.enabled=True is experimental.\n",
      "    \n",
      "    >>> l = [('Alice', 1)]\n",
      "    >>> spark.createDataFrame(l).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "    >>> spark.createDataFrame(d).collect()\n",
      "    [Row(age=1, name='Alice')]\n",
      "    \n",
      "    >>> rdd = sc.parallelize(l)\n",
      "    >>> spark.createDataFrame(rdd).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      "    >>> df.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> Person = Row('name', 'age')\n",
      "    >>> person = rdd.map(lambda r: Person(*r))\n",
      "    >>> df2 = spark.createDataFrame(person)\n",
      "    >>> df2.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql.types import *\n",
      "    >>> schema = StructType([\n",
      "    ...    StructField(\"name\", StringType(), True),\n",
      "    ...    StructField(\"age\", IntegerType(), True)])\n",
      "    >>> df3 = spark.createDataFrame(rdd, schema)\n",
      "    >>> df3.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "    [Row(name='Alice', age=1)]\n",
      "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "    [Row(0=1, 1=2)]\n",
      "    \n",
      "    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "    [Row(a='Alice', b=1)]\n",
      "    >>> rdd = rdd.map(lambda row: row[1])\n",
      "    >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      "    [Row(value=1)]\n",
      "    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    Py4JJavaError: ...\n",
      "    \n",
      "    .. versionadded:: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = spark.createDataFrame(data, ('last_name', 'first_name', 'ssn', 'occupation', 'age'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Let's see what type `spark.createDataFrame()` returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a look at the DataFrame's schema and some of its rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+--------------------+---+\n",
      "| last_name|first_name|        ssn|          occupation|age|\n",
      "+----------+----------+-----------+--------------------+---+\n",
      "|     Brown|     Jason|182-83-5988|Building services...|  2|\n",
      "|     Brown|      Cody|298-53-9877|Dance movement ps...|  3|\n",
      "| Hendricks|   Jessica|461-01-1867|Learning disabili...| 18|\n",
      "|     Baker|     Scott|603-81-3455|Maintenance engineer| 18|\n",
      "|   Alvarez|     Sarah|304-30-5738|          Geochemist| 40|\n",
      "|Montgomery|    George|200-45-4002|      Phytotherapist| 27|\n",
      "|    Brooks|     Jesse|674-90-7121|  Secretary, company| 27|\n",
      "|     Davis|   Valerie|093-81-5058|          Aid worker| 34|\n",
      "|  Johnston|    Andrea|405-71-6928|Engineer, production| 40|\n",
      "| Zimmerman|      Mark|285-09-1368|  Personal assistant|  7|\n",
      "|     Drake|     Sheri|432-75-9180|Biomedical scientist|  6|\n",
      "|  Campbell| Stephanie|287-31-8774|Chartered certifi...| 15|\n",
      "|    Murphy|    Steven|885-84-0932|Commercial/reside...| 25|\n",
      "|  Andersen|   Lindsay|299-52-4282|Administrator, lo...| 39|\n",
      "|     Clark|   Claudia|572-15-9413|Insurance risk su...| 20|\n",
      "|  Anderson|   Michael|592-79-1854|   Company secretary| 37|\n",
      "|    Carter|   Bradley|750-85-4885|Armed forces logi...| 36|\n",
      "|     Lyons|     Chris|795-90-8059|          Astronomer| 36|\n",
      "|   Gregory| Stephanie|800-68-1097|          Orthoptist| 36|\n",
      "|    Garcia|      Anna|528-14-5681|          Astronomer| 29|\n",
      "+----------+----------+-----------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3d) Create a Spark Dataframe from a source\n",
    "\n",
    "#### Another way to create Spark Dataframes is using the `read` method that allows you to read in data from sources like HDFS, local file storage, but also external DBs like MongoDB, Cassandra, PostgreSQL, etc.. The `read` method does natively support Parquet, Avro and all other well-known file formats like csv, xml, json, etc.\n",
    "\n",
    "#### As specified, Spark can also read from external storage frameworks like MongoDB, Cassandra, PostgreSQL, etc. For reading in data from these sources, specific connectors are created. Most of them can be found and are well documented on the [_Spark Packages site_](https://spark-packages.org/), which is the equivalent of to [_R Packages site_](https://cran.r-project.org/) for Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2| setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2| setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1| setosa|\n",
      "|         4.3|        3.0|         1.1|        0.1| setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2| setosa|\n",
      "|         5.7|        4.4|         1.5|        0.4| setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4| setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3| setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3| setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading from local file\n",
    "\n",
    "irisDF = spark.read.option(\"header\", True).option(\"sep\", \",\").csv(\"/data/iris.csv\")\n",
    "irisDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3e) Transform DF to rdd and find out how many partitions will the DataFrame be split into?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A note about DataFrames and queries\n",
    "\n",
    "When you use DataFrames or Spark SQL, you are building up a _query plan_. Each transformation you apply to a DataFrame adds some information to the query plan. When you finally call an action, which triggers execution of your Spark job, several things happen:\n",
    "\n",
    "1. Spark's Catalyst optimizer analyzes the query plan (called an _unoptimized logical query plan_) and attempts to optimize it. Optimizations include (but aren't limited to) rearranging and combining `filter()` operations for efficiency, converting `Decimal` operations to more efficient long integer operations, and pushing some operations down into the data source (e.g., a `filter()` operation might be translated to a SQL `WHERE` clause, if the data source is a traditional SQL RDBMS). The result of this optimization phase is an _optimized logical plan_.\n",
    "2. Once Catalyst has an optimized logical plan, it then constructs multiple _physical_ plans from it. Specifically, it implements the query in terms of lower level Spark RDD operations.\n",
    "3. Catalyst chooses which physical plan to use via _cost optimization_. That is, it determines which physical plan is the most efficient (or least expensive), and uses that one.\n",
    "4. Finally, once the physical RDD execution plan is established, Spark actually executes the job.\n",
    "\n",
    "You can examine the query plan using the `explain()` function on a DataFrame. By default, `explain()` only shows you the final physical plan; however, if you pass it an argument of `True`, it will show you all phases.\n",
    "\n",
    "(If you want to take a deeper dive into how Catalyst optimizes DataFrame queries, this blog post, while a little old, is an excellent overview: [Deep Dive into Spark SQL's Catalyst Optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html).)\n",
    "\n",
    "Let's add a couple transformations to our DataFrame and look at the query plan on the resulting transformed DataFrame. Don't be too concerned if it looks like gibberish. As you gain more experience with Apache Spark, you'll begin to be able to use `explain()` to help you understand more about your DataFrame operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+--------------------+---+\n",
      "| last_name|first_name|        ssn|          occupation|age|\n",
      "+----------+----------+-----------+--------------------+---+\n",
      "|     Brown|     Jason|182-83-5988|Building services...|  1|\n",
      "|     Brown|      Cody|298-53-9877|Dance movement ps...|  2|\n",
      "| Hendricks|   Jessica|461-01-1867|Learning disabili...| 17|\n",
      "|     Baker|     Scott|603-81-3455|Maintenance engineer| 17|\n",
      "|   Alvarez|     Sarah|304-30-5738|          Geochemist| 39|\n",
      "|Montgomery|    George|200-45-4002|      Phytotherapist| 26|\n",
      "|    Brooks|     Jesse|674-90-7121|  Secretary, company| 26|\n",
      "|     Davis|   Valerie|093-81-5058|          Aid worker| 33|\n",
      "|  Johnston|    Andrea|405-71-6928|Engineer, production| 39|\n",
      "| Zimmerman|      Mark|285-09-1368|  Personal assistant|  6|\n",
      "|     Drake|     Sheri|432-75-9180|Biomedical scientist|  5|\n",
      "|  Campbell| Stephanie|287-31-8774|Chartered certifi...| 14|\n",
      "|    Murphy|    Steven|885-84-0932|Commercial/reside...| 24|\n",
      "|  Andersen|   Lindsay|299-52-4282|Administrator, lo...| 38|\n",
      "|     Clark|   Claudia|572-15-9413|Insurance risk su...| 19|\n",
      "|  Anderson|   Michael|592-79-1854|   Company secretary| 36|\n",
      "|    Carter|   Bradley|750-85-4885|Armed forces logi...| 35|\n",
      "|     Lyons|     Chris|795-90-8059|          Astronomer| 35|\n",
      "|   Gregory| Stephanie|800-68-1097|          Orthoptist| 35|\n",
      "|    Garcia|      Anna|528-14-5681|          Astronomer| 28|\n",
      "+----------+----------+-----------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform dataDF through a select transformation and rename the newly created '(age -1)' column to 'age'\n",
    "# Because select is a transformation and Spark uses lazy evaluation, no jobs, stages,\n",
    "# or tasks will be launched when we run this code.\n",
    "subDF = dataDF.select('last_name', 'first_name', 'ssn', 'occupation',(dataDF.age - 1).alias('age'))\n",
    "subDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a look at the query plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('last_name, None), unresolvedalias('first_name, None), unresolvedalias('ssn, None), unresolvedalias('occupation, None), (age#4L - 1) AS age#253]\n",
      "+- LogicalRDD [last_name#0, first_name#1, ssn#2, occupation#3, age#4L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "last_name: string, first_name: string, ssn: string, occupation: string, age: bigint\n",
      "Project [last_name#0, first_name#1, ssn#2, occupation#3, (age#4L - cast(1 as bigint)) AS age#253L]\n",
      "+- LogicalRDD [last_name#0, first_name#1, ssn#2, occupation#3, age#4L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [last_name#0, first_name#1, ssn#2, occupation#3, (age#4L - 1) AS age#253L]\n",
      "+- LogicalRDD [last_name#0, first_name#1, ssn#2, occupation#3, age#4L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [last_name#0, first_name#1, ssn#2, occupation#3, (age#4L - 1) AS age#253L]\n",
      "+- Scan ExistingRDD[last_name#0,first_name#1,ssn#2,occupation#3,age#4L]\n"
     ]
    }
   ],
   "source": [
    "subDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3f) Use _collect_ to view results\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3d.png\" style=\"height:700px\"/>\n",
    "\n",
    "#### To see a list of elements decremented by one, we need to create a new list on the driver from the the data distributed in the executor nodes.  To do this we can call the `collect()` method on our DataFrame.  `collect()` is often used after transformations to ensure that we are only returning a *small* amount of data to the driver.  This is done because the data returned to the driver must fit into the driver's available memory.  If not, the driver will crash.\n",
    "\n",
    "#### The `collect()` method is the first action operation that we have encountered.  Action operations cause Spark to perform the (lazy) transformation operations that are required to compute the values returned by the action.  In our example, this means that tasks will now be launched to perform the `createDataFrame`, `select`, and `collect` operations.\n",
    "\n",
    "#### In the diagram, the dataset is broken into four partitions, so four `collect()` tasks are launched. Each task collects the entries in its partition and sends the result to the driver, which creates a list of the values, as shown in the figure below.\n",
    " \n",
    "#### Now let's run `collect()` on `subDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(last_name='Brown', first_name='Jason', ssn='182-83-5988', occupation='Building services engineer', age=1), Row(last_name='Brown', first_name='Cody', ssn='298-53-9877', occupation='Dance movement psychotherapist', age=2)]\n"
     ]
    }
   ],
   "source": [
    "# Let's collect the data\n",
    "results = subDF.take(2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A better way to visualize the data is to use the `show()` method. If you don't tell `show()` how many rows to display, it displays 20 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+--------------------+---+\n",
      "| last_name|first_name|        ssn|          occupation|age|\n",
      "+----------+----------+-----------+--------------------+---+\n",
      "|     Brown|     Jason|182-83-5988|Building services...|  1|\n",
      "|     Brown|      Cody|298-53-9877|Dance movement ps...|  2|\n",
      "| Hendricks|   Jessica|461-01-1867|Learning disabili...| 17|\n",
      "|     Baker|     Scott|603-81-3455|Maintenance engineer| 17|\n",
      "|   Alvarez|     Sarah|304-30-5738|          Geochemist| 39|\n",
      "|Montgomery|    George|200-45-4002|      Phytotherapist| 26|\n",
      "|    Brooks|     Jesse|674-90-7121|  Secretary, company| 26|\n",
      "|     Davis|   Valerie|093-81-5058|          Aid worker| 33|\n",
      "|  Johnston|    Andrea|405-71-6928|Engineer, production| 39|\n",
      "| Zimmerman|      Mark|285-09-1368|  Personal assistant|  6|\n",
      "|     Drake|     Sheri|432-75-9180|Biomedical scientist|  5|\n",
      "|  Campbell| Stephanie|287-31-8774|Chartered certifi...| 14|\n",
      "|    Murphy|    Steven|885-84-0932|Commercial/reside...| 24|\n",
      "|  Andersen|   Lindsay|299-52-4282|Administrator, lo...| 38|\n",
      "|     Clark|   Claudia|572-15-9413|Insurance risk su...| 19|\n",
      "|  Anderson|   Michael|592-79-1854|   Company secretary| 36|\n",
      "|    Carter|   Bradley|750-85-4885|Armed forces logi...| 35|\n",
      "|     Lyons|     Chris|795-90-8059|          Astronomer| 35|\n",
      "|   Gregory| Stephanie|800-68-1097|          Orthoptist| 35|\n",
      "|    Garcia|      Anna|528-14-5681|          Astronomer| 28|\n",
      "+----------+----------+-----------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you'd prefer that `show()` not truncate the data, you can tell it not to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------------------------------------------------+---+\n",
      "|last_name |first_name |ssn        |occupation                                           |age|\n",
      "+----------+-----------+-----------+-----------------------------------------------------+---+\n",
      "|Brown     |Jason      |182-83-5988|Building services engineer                           |1  |\n",
      "|Brown     |Cody       |298-53-9877|Dance movement psychotherapist                       |2  |\n",
      "|Hendricks |Jessica    |461-01-1867|Learning disability nurse                            |17 |\n",
      "|Baker     |Scott      |603-81-3455|Maintenance engineer                                 |17 |\n",
      "|Alvarez   |Sarah      |304-30-5738|Geochemist                                           |39 |\n",
      "|Montgomery|George     |200-45-4002|Phytotherapist                                       |26 |\n",
      "|Brooks    |Jesse      |674-90-7121|Secretary, company                                   |26 |\n",
      "|Davis     |Valerie    |093-81-5058|Aid worker                                           |33 |\n",
      "|Johnston  |Andrea     |405-71-6928|Engineer, production                                 |39 |\n",
      "|Zimmerman |Mark       |285-09-1368|Personal assistant                                   |6  |\n",
      "|Drake     |Sheri      |432-75-9180|Biomedical scientist                                 |5  |\n",
      "|Campbell  |Stephanie  |287-31-8774|Chartered certified accountant                       |14 |\n",
      "|Murphy    |Steven     |885-84-0932|Commercial/residential surveyor                      |24 |\n",
      "|Andersen  |Lindsay    |299-52-4282|Administrator, local government                      |38 |\n",
      "|Clark     |Claudia    |572-15-9413|Insurance risk surveyor                              |19 |\n",
      "|Anderson  |Michael    |592-79-1854|Company secretary                                    |36 |\n",
      "|Carter    |Bradley    |750-85-4885|Armed forces logistics/support/administrative officer|35 |\n",
      "|Lyons     |Chris      |795-90-8059|Astronomer                                           |35 |\n",
      "|Gregory   |Stephanie  |800-68-1097|Orthoptist                                           |35 |\n",
      "|Garcia    |Anna       |528-14-5681|Astronomer                                           |28 |\n",
      "|Kelly     |Cynthia    |406-47-0397|Education administrator                              |41 |\n",
      "|Duncan    |Christopher|323-89-6183|Data processing manager                              |23 |\n",
      "|Lloyd     |Tony       |024-16-5871|Office manager                                       |34 |\n",
      "|Martin    |Jeanette   |369-71-3206|Historic buildings inspector/conservation officer    |29 |\n",
      "|Rose      |Daniel     |839-51-7270|Photographer                                         |40 |\n",
      "|Graves    |Samuel     |191-92-9664|Administrator, local government                      |2  |\n",
      "|Brown     |Jo         |815-54-2061|Sports development officer                           |30 |\n",
      "|Hickman   |Scott      |282-67-0568|Architectural technologist                           |2  |\n",
      "|Terry     |Katherine  |159-37-9976|Education administrator                              |41 |\n",
      "|Bryant    |Logan      |626-89-4666|Tourism officer                                      |40 |\n",
      "+----------+-----------+-----------+-----------------------------------------------------+---+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subDF.show(n=30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3g) Use _count_ to get total\n",
    "\n",
    "#### One of the most basic jobs that we can run is the `count()` job which will count the number of elements in a DataFrame, using the `count()` action. Since `select()` creates a new DataFrame with the same number of elements as the starting DataFrame, we expect that applying `count()` to each DataFrame will return the same result.\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3e.png\" style=\"height:700px\"/>\n",
    "\n",
    "#### Note that because `count()` is an action operation, if we had not already performed an action with `collect()`, then Spark would now perform the transformation operations when we executed `count()`.\n",
    "\n",
    "#### Each task counts the entries in its partition and sends the result to your SparkContext, which adds up all of the counts. The figure on the right shows what would happen if we ran `count()` on a small example dataset with just four partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(dataDF.count())\n",
    "print(subDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3f) Apply transformation _filter_ and view results with _show_\n",
    "\n",
    "#### Next, we'll create a new DataFrame that only contains the people whose ages are less than 10. To do this, we'll use the `filter()` transformation. (You can also use `where()`, an alias for `filter()`, if you prefer something more SQL-like). The `filter()` method is a transformation operation that creates a new DataFrame from the input DataFrame, keeping only values that match the filter expression.\n",
    "\n",
    "#### The figure shows how this might work on the small four-partition dataset.\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3f.png\" style=\"height:700px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-------------------------------------------------+---+\n",
      "|last_name |first_name|ssn        |occupation                                       |age|\n",
      "+----------+----------+-----------+-------------------------------------------------+---+\n",
      "|Brown     |Jason     |182-83-5988|Building services engineer                       |1  |\n",
      "|Brown     |Cody      |298-53-9877|Dance movement psychotherapist                   |2  |\n",
      "|Zimmerman |Mark      |285-09-1368|Personal assistant                               |6  |\n",
      "|Drake     |Sheri     |432-75-9180|Biomedical scientist                             |5  |\n",
      "|Graves    |Samuel    |191-92-9664|Administrator, local government                  |2  |\n",
      "|Hickman   |Scott     |282-67-0568|Architectural technologist                       |2  |\n",
      "|King      |Matthew   |263-65-2624|Psychologist, prison and probation services      |7  |\n",
      "|Lee       |Christina |072-60-9564|Music tutor                                      |9  |\n",
      "|Jones     |Timothy   |215-04-0471|Dispensing optician                              |1  |\n",
      "|Burns     |Roy       |200-09-8924|Commercial art gallery manager                   |0  |\n",
      "|Gay       |John      |166-25-3954|Risk analyst                                     |2  |\n",
      "|Mckinney  |Jared     |070-71-1782|Early years teacher                              |1  |\n",
      "|Russell   |Sandra    |717-41-8980|Radio broadcast assistant                        |1  |\n",
      "|Branch    |Jose      |552-48-7277|Production assistant, radio                      |1  |\n",
      "|Hall      |Belinda   |278-41-1849|Surveyor, building control                       |3  |\n",
      "|Hill      |Chelsea   |695-10-4686|Research scientist (maths)                       |5  |\n",
      "|Harrington|James     |475-20-7259|Historic buildings inspector/conservation officer|3  |\n",
      "|Newman    |Michael   |845-68-5707|Clinical psychologist                            |0  |\n",
      "|Cruz      |Lisa      |058-92-8829|Engineer, drilling                               |8  |\n",
      "|Mitchell  |Michael   |179-13-8207|International aid/development worker             |5  |\n",
      "+----------+----------+-----------+-------------------------------------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2472"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filteredDF = subDF.filter(subDF.age < 10)\n",
    "filteredDF.show(truncate=False)\n",
    "filteredDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternetively you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2472\n"
     ]
    }
   ],
   "source": [
    "filteredDF = subDF.filter(\"age < 10\")\n",
    "print(filteredDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3h) Write results to file\n",
    "\n",
    "#### In part (3d) is  shown how a file is read in. Similarly, you can write a file to an external local file, file system (like Hadoop) or external storage framework.\n",
    "\n",
    "#### By default Spark expects a connection to hdfs, but also a local file location can be specified. Additionally, by defaults, Spark writes a Parquet file. But also other popular file formats like Avro, csv, xml, or json could be specified.\n",
    "\n",
    "#### As specified, Spark can also write to external storage frameworks like MongoDB, Cassandra, PostgreSQL, etc. For writing to data these sources, specific connectors are created. Most of them can be found and are well documented on the [_Spark Packages site_](https://spark-packages.org/), which is the equivalent of to [_R Packages site_](https://cran.r-project.org/) for Spark.\n",
    "\n",
    "#### When writing a Spark dataframe to disk, this is always done distributed. This means that a directory is create (on each machine in the cluster) and whitin this directory partitions assinged to the node are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write to local file (Parquet)\n",
    "\n",
    "fileName = \"file:///data/output/iris_out\"\n",
    "\n",
    "filteredDF.write.save(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write to local file (csv)\n",
    "filteredDF.write.csv(fileName + \"_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** Note: You can see that 4 partitions are created. Which might not always be desired if you only need 1 single file. To save just one csv file, you can repartition the DataFrame before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write to local file (csv): 1 partition\n",
    "filteredDF.repartition(1).write.csv(fileName + \"_csv_one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasDF = filteredDF.toPandas()\n",
    "\n",
    "pandasDF.to_csv(\"/data/output/pandasDF.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Python Lambda functions and User Defined Functions\n",
    "\n",
    "#### Python supports the use of small one-line anonymous functions that are not bound to a name at runtime.\n",
    "\n",
    "#### `lambda` functions, borrowed from LISP, can be used wherever function objects are required. They are syntactically restricted to a single expression. Remember that `lambda` functions are a matter of style and using them is never required - semantically, they are just syntactic sugar for a normal function definition. You can always define a separate normal function instead, but using a `lambda` function is an equivalent and more compact form of coding. Ideally you should consider using `lambda` functions where you want to encapsulate non-reusable code without littering your code with one-line functions.\n",
    "\n",
    "#### Here, instead of defining a separate function for the `filter()` transformation, we will use an inline `lambda()` function and we will register that lambda as a Spark _User Defined Function_ (UDF). A UDF is a special wrapper around a function, allowing the function to be used in a DataFrame query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = filter(lambda a: a < 10, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+--------------------+---+\n",
      "| last_name|first_name|        ssn|          occupation|age|\n",
      "+----------+----------+-----------+--------------------+---+\n",
      "|     Brown|     Jason|182-83-5988|Building services...|  2|\n",
      "|     Brown|      Cody|298-53-9877|Dance movement ps...|  3|\n",
      "| Zimmerman|      Mark|285-09-1368|  Personal assistant|  7|\n",
      "|     Drake|     Sheri|432-75-9180|Biomedical scientist|  6|\n",
      "|    Graves|    Samuel|191-92-9664|Administrator, lo...|  3|\n",
      "|   Hickman|     Scott|282-67-0568|Architectural tec...|  3|\n",
      "|      King|   Matthew|263-65-2624|Psychologist, pri...|  8|\n",
      "|     Jones|   Timothy|215-04-0471| Dispensing optician|  2|\n",
      "|     Burns|       Roy|200-09-8924|Commercial art ga...|  1|\n",
      "|       Gay|      John|166-25-3954|        Risk analyst|  3|\n",
      "|  Mckinney|     Jared|070-71-1782| Early years teacher|  2|\n",
      "|   Russell|    Sandra|717-41-8980|Radio broadcast a...|  2|\n",
      "|    Branch|      Jose|552-48-7277|Production assist...|  2|\n",
      "|      Hall|   Belinda|278-41-1849|Surveyor, buildin...|  4|\n",
      "|      Hill|   Chelsea|695-10-4686|Research scientis...|  6|\n",
      "|Harrington|     James|475-20-7259|Historic building...|  4|\n",
      "|    Newman|   Michael|845-68-5707|Clinical psycholo...|  1|\n",
      "|      Cruz|      Lisa|058-92-8829|  Engineer, drilling|  9|\n",
      "|  Mitchell|   Michael|179-13-8207|International aid...|  6|\n",
      "|    Miller|     James|008-77-3552|Scientist, resear...|  2|\n",
      "+----------+----------+-----------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2262"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "less_ten = udf(lambda s: s < 10, BooleanType())\n",
    "lambdaDF = dataDF.filter(less_ten(dataDF.age))\n",
    "lambdaDF.show()\n",
    "lambdaDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+--------------------+---+\n",
      "| last_name|first_name|        ssn|          occupation|age|\n",
      "+----------+----------+-----------+--------------------+---+\n",
      "|     Brown|     Jason|182-83-5988|Building services...|  2|\n",
      "|     Drake|     Sheri|432-75-9180|Biomedical scientist|  6|\n",
      "|      King|   Matthew|263-65-2624|Psychologist, pri...|  8|\n",
      "|     Jones|   Timothy|215-04-0471| Dispensing optician|  2|\n",
      "|  Mckinney|     Jared|070-71-1782| Early years teacher|  2|\n",
      "|   Russell|    Sandra|717-41-8980|Radio broadcast a...|  2|\n",
      "|    Branch|      Jose|552-48-7277|Production assist...|  2|\n",
      "|      Hall|   Belinda|278-41-1849|Surveyor, buildin...|  4|\n",
      "|      Hill|   Chelsea|695-10-4686|Research scientis...|  6|\n",
      "|Harrington|     James|475-20-7259|Historic building...|  4|\n",
      "|  Mitchell|   Michael|179-13-8207|International aid...|  6|\n",
      "|    Miller|     James|008-77-3552|Scientist, resear...|  2|\n",
      "|   Alvarez|   Matthew|266-76-2718|Engineer, broadca...|  4|\n",
      "|  Sullivan|     Diana|278-99-2904|Chief Executive O...|  2|\n",
      "|      Long|      Cody|352-67-2941|          Translator|  4|\n",
      "|   Ramirez|    Daniel|183-19-0892|Commercial hortic...|  4|\n",
      "|    Walker|     Megan|863-90-8902|Engineer, civil (...|  6|\n",
      "|   Herrera|      Lisa|158-19-7870|  Editor, film/video|  8|\n",
      "|   Stewart|    Dustin|178-24-9180|Psychologist, for...|  2|\n",
      "|     Smith|   Phillip|485-69-9746|Surveyor, hydrogr...|  4|\n",
      "+----------+----------+-----------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1090"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's show the even values less than 10\n",
    "even = udf(lambda s: s % 2 == 0, BooleanType())\n",
    "evenDF = lambdaDF.filter(even(lambdaDF.age))\n",
    "evenDF.show()\n",
    "evenDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+--------------------+---+\n",
      "| last_name|first_name|        ssn|          occupation|age|\n",
      "+----------+----------+-----------+--------------------+---+\n",
      "|     Brown|     Jason|182-83-5988|Building services...|  1|\n",
      "|     Brown|      Cody|298-53-9877|Dance movement ps...|  2|\n",
      "| Zimmerman|      Mark|285-09-1368|  Personal assistant|  6|\n",
      "|     Drake|     Sheri|432-75-9180|Biomedical scientist|  5|\n",
      "|    Graves|    Samuel|191-92-9664|Administrator, lo...|  2|\n",
      "|   Hickman|     Scott|282-67-0568|Architectural tec...|  2|\n",
      "|      King|   Matthew|263-65-2624|Psychologist, pri...|  7|\n",
      "|       Lee| Christina|072-60-9564|         Music tutor|  9|\n",
      "|     Jones|   Timothy|215-04-0471| Dispensing optician|  1|\n",
      "|     Burns|       Roy|200-09-8924|Commercial art ga...|  0|\n",
      "|       Gay|      John|166-25-3954|        Risk analyst|  2|\n",
      "|  Mckinney|     Jared|070-71-1782| Early years teacher|  1|\n",
      "|   Russell|    Sandra|717-41-8980|Radio broadcast a...|  1|\n",
      "|    Branch|      Jose|552-48-7277|Production assist...|  1|\n",
      "|      Hall|   Belinda|278-41-1849|Surveyor, buildin...|  3|\n",
      "|      Hill|   Chelsea|695-10-4686|Research scientis...|  5|\n",
      "|Harrington|     James|475-20-7259|Historic building...|  3|\n",
      "|    Newman|   Michael|845-68-5707|Clinical psycholo...|  0|\n",
      "|      Cruz|      Lisa|058-92-8829|  Engineer, drilling|  8|\n",
      "|  Mitchell|   Michael|179-13-8207|International aid...|  5|\n",
      "+----------+----------+-----------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def brokenTen(value):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        value (int): A number.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether `value` is less than ten.\n",
    "    \"\"\"\n",
    "    if (value < 10):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "btUDF = udf(brokenTen)\n",
    "brokenDF = subDF.filter(btUDF(subDF.age) == True)\n",
    "brokenDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Additional DataFrame actions\n",
    "\n",
    "#### Let's investigate some additional actions:\n",
    "\n",
    "* [first()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.first)\n",
    "* [take()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.take)\n",
    "\n",
    "#### MAGIC One useful thing to do when we have a new dataset is to look at the first few entries to obtain a rough idea of what information is available.  In Spark, we can do that using actions like `first()`, `take()`, and `show()`. Note that for the `first()` and `take()` actions, the elements that are returned depend on how the DataFrame is *partitioned*.\n",
    "\n",
    "#### MAGIC Instead of using the `collect()` action, we can use the `take(n)` action to return the first _n_ elements of the DataFrame. The `first()` action returns the first element of a DataFrame, and is equivalent to `take(1)[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first: Row(last_name='Brown', first_name='Jason', ssn='182-83-5988', occupation='Building services engineer', age=1)\n"
     ]
    }
   ],
   "source": [
    "print(\"first: \" + str(filteredDF.first()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four of them: [Row(last_name='Brown', first_name='Jason', ssn='182-83-5988', occupation='Building services engineer', age=1), Row(last_name='Brown', first_name='Cody', ssn='298-53-9877', occupation='Dance movement psychotherapist', age=2), Row(last_name='Zimmerman', first_name='Mark', ssn='285-09-1368', occupation='Personal assistant', age=6), Row(last_name='Drake', first_name='Sheri', ssn='432-75-9180', occupation='Biomedical scientist', age=5)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Four of them: \" + str(filteredDF.take(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Additional DataFrame transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6a) _orderBy_\n",
    "\n",
    "####[`orderBy()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct) allows you to sort a DataFrame by one or more columns, producing a new DataFrame.\n",
    "\n",
    "#### For example, let's get the first five oldest people in the original (unfiltered) DataFrame. We can use the `orderBy()` transformation. `orderBy` takes one or more columns, either as _names_ (strings) or as `Column` objects. To get a `Column` object, we use one of two notations on the DataFrame:\n",
    "\n",
    "#### * Pandas-style notation: `filteredDF.age`\n",
    "#### * Subscript notation: `filteredDF['age']`\n",
    "\n",
    "#### Both of those syntaxes return a `Column`, which has additional methods like `desc()` (for sorting in descending order) or `asc()` (for sorting in ascending order, which is the default).\n",
    "\n",
    "#### Here are some examples:\n",
    "\n",
    "```\n",
    "dataDF.orderBy(dataDF['age'])  # sort by age in ascending order; returns a new DataFrame\n",
    "dataDF.orderBy(dataDF.last_name.desc()) # sort by last name in descending order\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+--------------------+---+\n",
      "|last_name|first_name|        ssn|          occupation|age|\n",
      "+---------+----------+-----------+--------------------+---+\n",
      "|  Ramirez|       Roy|391-25-9789|Conservation offi...| 47|\n",
      "|    Foley|    Gerald|337-87-0606|             Curator| 47|\n",
      "|  Johnson|   Vincent|809-40-1297|Intelligence analyst| 47|\n",
      "|    Hobbs|     Jaime|210-13-4378|         Ship broker| 47|\n",
      "|    Smith|   Kristen|205-33-2744|          Contractor| 47|\n",
      "+---------+----------+-----------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the five oldest people in the list. To do that, sort by age in descending order.\n",
    "dataDF.orderBy(dataDF.age.desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's reverse the sort order. Since ascending sort is the default, we can actually use a `Column` object expression or a simple string, in this case. The `desc()` and `asc()` methods are only defined on `Column`. Something like `orderBy('age'.desc())` would not work, because there's no `desc()` method on Python string objects. That's why we needed the column expression. But if we're just using the defaults, we can pass a string column name into `orderBy()`. This is sometimes easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+--------------------+---+\n",
      "|last_name|first_name|        ssn|          occupation|age|\n",
      "+---------+----------+-----------+--------------------+---+\n",
      "|   Newman|   Michael|845-68-5707|Clinical psycholo...|  1|\n",
      "|Mccormick|     David|139-63-5401|Logistics and dis...|  1|\n",
      "|   Morris| Stephanie|143-50-7887|Regulatory affair...|  1|\n",
      "|     Hall|     Renee|765-37-8660| Mental health nurse|  1|\n",
      "|   Burton|     Holly|574-81-9974|Glass blower/desi...|  1|\n",
      "+---------+----------+-----------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.orderBy('age').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6b) _distinct_ and _dropDuplicates_\n",
    "\n",
    "#### [`distinct()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct) filters out duplicate rows, and it considers all columns. Since our data is completely randomly generated (by `fake-factory`), it's extremely unlikely that there are any duplicate rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "| 21|\n",
      "| 22|\n",
      "| 23|\n",
      "| 24|\n",
      "| 25|\n",
      "| 26|\n",
      "| 27|\n",
      "| 28|\n",
      "| 29|\n",
      "| 30|\n",
      "| 31|\n",
      "| 32|\n",
      "| 33|\n",
      "| 34|\n",
      "| 35|\n",
      "| 36|\n",
      "| 37|\n",
      "| 38|\n",
      "| 39|\n",
      "| 40|\n",
      "| 41|\n",
      "| 42|\n",
      "| 43|\n",
      "| 44|\n",
      "| 45|\n",
      "| 46|\n",
      "| 47|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataDF.count())\n",
    "dataDF.select(\"age\").distinct().orderBy('age').show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To demonstrate `distinct()`, let's create a quick throwaway dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDF = spark.createDataFrame([(\"Joe\", 1), (\"Joe\", 1), (\"Anna\", 15), (\"Anna\", 12), (\"Ravi\", 5)], ('name', 'score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|score|\n",
      "+----+-----+\n",
      "| Joe|    1|\n",
      "| Joe|    1|\n",
      "|Anna|   15|\n",
      "|Anna|   12|\n",
      "|Ravi|    5|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|score|\n",
      "+----+-----+\n",
      "| Joe|    1|\n",
      "|Ravi|    5|\n",
      "|Anna|   12|\n",
      "|Anna|   15|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that one of the (\"Joe\", 1) rows was deleted, but both rows with name \"Anna\" were kept, because all columns in a row must match another row for it to be considered a duplicate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [`dropDuplicates()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropDuplicates) is like `distinct()`, except that it allows us to specify the columns to compare. For instance, we can use it to drop all rows where the first name and last name duplicates (ignoring the occupation and age columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "9321\n"
     ]
    }
   ],
   "source": [
    "print(dataDF.count())\n",
    "print(dataDF.dropDuplicates(['first_name', 'last_name']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6c) _drop_\n",
    " \n",
    "#### [`drop()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.drop) is like the opposite of `select()`: Instead of selecting specific columns from a DataFrame, it drops a specifed column from a DataFrame.\n",
    "\n",
    "####  Here's a simple use case: Suppose you're reading from a 1,000-column CSV file, and you have to get rid of five of the columns. Instead of selecting 995 of the columns, it's easier just to drop the five you don't want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+\n",
      "| last_name|first_name|        ssn|\n",
      "+----------+----------+-----------+\n",
      "|     Brown|     Jason|182-83-5988|\n",
      "|     Brown|      Cody|298-53-9877|\n",
      "| Hendricks|   Jessica|461-01-1867|\n",
      "|     Baker|     Scott|603-81-3455|\n",
      "|   Alvarez|     Sarah|304-30-5738|\n",
      "|Montgomery|    George|200-45-4002|\n",
      "|    Brooks|     Jesse|674-90-7121|\n",
      "|     Davis|   Valerie|093-81-5058|\n",
      "|  Johnston|    Andrea|405-71-6928|\n",
      "| Zimmerman|      Mark|285-09-1368|\n",
      "|     Drake|     Sheri|432-75-9180|\n",
      "|  Campbell| Stephanie|287-31-8774|\n",
      "|    Murphy|    Steven|885-84-0932|\n",
      "|  Andersen|   Lindsay|299-52-4282|\n",
      "|     Clark|   Claudia|572-15-9413|\n",
      "|  Anderson|   Michael|592-79-1854|\n",
      "|    Carter|   Bradley|750-85-4885|\n",
      "|     Lyons|     Chris|795-90-8059|\n",
      "|   Gregory| Stephanie|800-68-1097|\n",
      "|    Garcia|      Anna|528-14-5681|\n",
      "+----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.drop('occupation').drop('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6d) _groupBy_\n",
    "\n",
    "#### [`groupBy()`]((http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy) is one of the most powerful transformations. It allows you to perform aggregations on a DataFrame.\n",
    "\n",
    "#### Unlike other DataFrame transformations, `groupBy()` does _not_ return a DataFrame. Instead, it returns a special [GroupedData](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData) object that contains various aggregation functions.\n",
    "\n",
    "#### The most commonly used aggregation function is [count()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.count),\n",
    "#### but there are others (like [sum()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.sum), [max()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.max), and [avg()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.avg).\n",
    "\n",
    "#### These aggregation functions typically create a new column and return a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+-----+\n",
      "|occupation                                 |count|\n",
      "+-------------------------------------------+-----+\n",
      "|Surveyor, hydrographic                     |28   |\n",
      "|Surveyor, minerals                         |28   |\n",
      "|Psychologist, clinical                     |28   |\n",
      "|Tourist information centre manager         |27   |\n",
      "|Architectural technologist                 |26   |\n",
      "|Librarian, public                          |26   |\n",
      "|Air traffic controller                     |26   |\n",
      "|Product manager                            |26   |\n",
      "|Trading standards officer                  |25   |\n",
      "|Media planner                              |25   |\n",
      "|Company secretary                          |25   |\n",
      "|Insurance claims handler                   |25   |\n",
      "|Charity officer                            |25   |\n",
      "|Video editor                               |25   |\n",
      "|Armed forces training and education officer|24   |\n",
      "|Herbalist                                  |24   |\n",
      "|Environmental manager                      |24   |\n",
      "|Network engineer                           |24   |\n",
      "|Microbiologist                             |24   |\n",
      "|Buyer, industrial                          |24   |\n",
      "+-------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.groupBy('occupation').count().orderBy(\"count\", ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+------------------+\n",
      "|occupation                             |avg(age)          |\n",
      "+---------------------------------------+------------------+\n",
      "|Retail merchandiser                    |27.095238095238095|\n",
      "|Engineer, aeronautical                 |29.0              |\n",
      "|Diplomatic Services operational officer|21.714285714285715|\n",
      "|Designer, ceramics/pottery             |15.642857142857142|\n",
      "|Librarian, academic                    |18.615384615384617|\n",
      "|Catering manager                       |25.875            |\n",
      "|Early years teacher                    |19.235294117647058|\n",
      "|English as a second language teacher   |16.142857142857142|\n",
      "|Primary school teacher                 |20.09090909090909 |\n",
      "|Occupational hygienist                 |22.4              |\n",
      "|Patent examiner                        |21.41176470588235 |\n",
      "|Control and instrumentation engineer   |22.157894736842106|\n",
      "|Clinical molecular geneticist          |24.692307692307693|\n",
      "|Estate agent                           |23.095238095238095|\n",
      "|Applications developer                 |31.63157894736842 |\n",
      "|Transport planner                      |16.818181818181817|\n",
      "|Art therapist                          |23.944444444444443|\n",
      "|Stage manager                          |21.88235294117647 |\n",
      "|Adult nurse                            |21.08695652173913 |\n",
      "|Petroleum engineer                     |26.083333333333332|\n",
      "+---------------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.groupBy().avg('age').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also use `groupBy()` to do aother useful aggregations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum age: 47\n",
      "Minimum age: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum age: \" + str(dataDF.groupBy().max('age').first()[0]))\n",
    "print(\"Minimum age: \" + str(dataDF.groupBy().min('age').first()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6e) _sample_ (optional)\n",
    "\n",
    "#### When analyzing data, the [`sample()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sample) transformation is often quite useful. It returns a new DataFrame with a random sample of elements from the dataset.  It takes in a `withReplacement` argument, which specifies whether it is okay to randomly pick the same item multiple times from the parent DataFrame (so when `withReplacement=True`, you can get the same item back multiple times). It takes in a `fraction` parameter, which specifies the fraction elements in the dataset you want to return. (So a `fraction` value of `0.20` returns 20% of the elements in the DataFrame.) It also takes an optional `seed` parameter that allows you to specify a seed value for the random number generator, so that reproducible results can be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "957\n",
      "+---------+-----------+-----------+--------------------+---+\n",
      "|last_name| first_name|        ssn|          occupation|age|\n",
      "+---------+-----------+-----------+--------------------+---+\n",
      "|   Brooks|      Jesse|674-90-7121|  Secretary, company| 27|\n",
      "| Campbell|  Stephanie|287-31-8774|Chartered certifi...| 15|\n",
      "|  Gregory|  Stephanie|800-68-1097|          Orthoptist| 36|\n",
      "|    Kelly|    Cynthia|406-47-0397|Education adminis...| 42|\n",
      "|     King|    Matthew|263-65-2624|Psychologist, pri...|  8|\n",
      "| Matthews|       Ryan|117-36-4219|                Make| 41|\n",
      "|Hernandez|   Benjamin|730-95-2184|    Industrial buyer| 41|\n",
      "|   Garner|      Louis|844-93-4799|  Wellsite geologist| 39|\n",
      "| Johnston|    William|200-54-7515| Animal technologist| 37|\n",
      "|   Taylor|       Tina|283-17-5670|Biomedical scientist| 23|\n",
      "| Johnston|      Steve|547-49-1078|        Bonds trader| 44|\n",
      "|     Diaz|Christopher|083-24-1465|      Pilot, airline| 28|\n",
      "| Richards|      Sarah|138-14-9343|Trading standards...| 30|\n",
      "|   Mendez|    Bradley|688-49-6147|Arts development ...|  8|\n",
      "|      Lee|       Ryan|415-06-3785|English as a seco...|  5|\n",
      "|    Smith|     Shelby|381-66-0713|           Ecologist| 18|\n",
      "|  Roberts|    Cameron|553-33-6292|   Financial adviser|  8|\n",
      "|   Harris|      Diana|310-62-9455|Chartered legal e...|  2|\n",
      "|Alexander|       Jean|170-80-5933|Horticultural the...|  5|\n",
      "|   Bryant|    Tiffany|513-51-8267|     Publishing copy| 10|\n",
      "+---------+-----------+-----------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampledDF = dataDF.sample(withReplacement=False, fraction=0.10)\n",
    "print(sampledDF.count())\n",
    "sampledDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n"
     ]
    }
   ],
   "source": [
    "print(dataDF.sample(withReplacement=False, fraction=0.05).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6f) _withColumn_ and _withColumnRenamed_\n",
    "\n",
    "#### [withColumn()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumn) is a transformation that is used to create a new column. This can be a constant column with a fix value, a column based on (an)other column(s). When using _withColumn()_, a new column will be added as last column to the dataframe. The _withColumn_ transformation takes two arguments:\n",
    "```\n",
    "a string depicting the new column name\n",
    "a function describing how the column should be created (can be a predefined, lambda, or udf function)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first create a new column that combines _last name_ and _first name_ into a new variable _name_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to combine two strings\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def combineStrings(string1, string2):\n",
    "    return string1 + \" \" + string2\n",
    "\n",
    "udfCombineStrings = udf(combineStrings, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+--------------------+---+-----------------+\n",
      "|last_name|first_name|        ssn|          occupation|age|             name|\n",
      "+---------+----------+-----------+--------------------+---+-----------------+\n",
      "|    Brown|     Jason|182-83-5988|Building services...|  2|      Brown Jason|\n",
      "|    Brown|      Cody|298-53-9877|Dance movement ps...|  3|       Brown Cody|\n",
      "|Hendricks|   Jessica|461-01-1867|Learning disabili...| 18|Hendricks Jessica|\n",
      "|    Baker|     Scott|603-81-3455|Maintenance engineer| 18|      Baker Scott|\n",
      "|  Alvarez|     Sarah|304-30-5738|          Geochemist| 40|    Alvarez Sarah|\n",
      "+---------+----------+-----------+--------------------+---+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use withColumn\n",
    "nameDF = dataDF.withColumn(\"name\", udfCombineStrings(\"last_name\", \"first_name\"))\n",
    "nameDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [withColumnRenamed()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumnRenamed) returns a new DataFrame by renaming an existing column. This is a no-op if schema doesn’t contain the given column name. The transformation takes two string arguments. The first string is the name of an existing variable in the dataframe. The second argument is a string signifying the ne name of the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The variable _name_ might create confusion with *last_name*, so let's change it to *full_name*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+--------------------+---+-----------------+\n",
      "|last_name|first_name|        ssn|          occupation|age|        full_name|\n",
      "+---------+----------+-----------+--------------------+---+-----------------+\n",
      "|    Brown|     Jason|182-83-5988|Building services...|  2|      Brown Jason|\n",
      "|    Brown|      Cody|298-53-9877|Dance movement ps...|  3|       Brown Cody|\n",
      "|Hendricks|   Jessica|461-01-1867|Learning disabili...| 18|Hendricks Jessica|\n",
      "|    Baker|     Scott|603-81-3455|Maintenance engineer| 18|      Baker Scott|\n",
      "|  Alvarez|     Sarah|304-30-5738|          Geochemist| 40|    Alvarez Sarah|\n",
      "+---------+----------+-----------+--------------------+---+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nameDF = nameDF.withColumnRenamed(\"name\", \"full_name\")\n",
    "nameDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Beyond 1 Dataframe: _union_ and _joins_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7a) Union\n",
    "#### Imagine we want to stack two dataframes. To do this, we can use [union()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.union). union() returns a new DataFrame containing union of rows in this and another frame.\n",
    "\n",
    "#### Notice that the two dataframe needs to have the same schema to be able to union them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's us create two sample datasets of dataDF and union them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "# creation of samples\n",
    "sampleDF1 = dataDF.sample(False, 0.01)\n",
    "print(sampleDF1.count())\n",
    "sampleDF2 = dataDF.sample(False, 0.01)\n",
    "print(sampleDF2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Union\n",
    "unionedDF = sampleDF1.union(sampleDF2)\n",
    "unionedDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7b) Join\n",
    "#### Like most other programming frameworks, it is possible to join dataframes in Spark. Nevertheless some caution is advised. Spark is a distributed framework and a dataframe is devided into multiple partitions. When joining dataframe this means, the framework needs to go and find matching records accross multiple partitions and nodes within the clusters. In other words, a lot of shuffles need to be done. As network bandwith is limited, join can be expensive within a distributed framework like Spark. \n",
    "\n",
    "#### Spark does already a fairly good job in constructing a query plan of how to optimizing joins. Nevertheless, the advice is to think twice before executing a join. Some tips:\n",
    "* Try to keep the traffic across the network as limited as possible by filtering and sampling before the join operation.\n",
    "* Try to use inner joins instead of outer join as the later are for more resource intensive.\n",
    "* Try to put the smallest dataframe on the left of the join operation as this is more optimal in the query plan.\n",
    "\n",
    "#### Within Spark most [join()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join) options like inner, left, right, and outer are available. The trainsformation takes at least 3 arguments:\n",
    "* A dataframe to join with the first dataframe\n",
    "* A column or expression use as variable to match both dataframes\n",
    "* An string saying how the join() needs to be done. Possible join types can be found [here](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's get started by creating two dataframes. One only containing _ssn_ and _name_ information and a second containing _ssn_ and _occupation_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+\n",
      "|        ssn| last_name|first_name|\n",
      "+-----------+----------+----------+\n",
      "|182-83-5988|     Brown|     Jason|\n",
      "|298-53-9877|     Brown|      Cody|\n",
      "|461-01-1867| Hendricks|   Jessica|\n",
      "|603-81-3455|     Baker|     Scott|\n",
      "|304-30-5738|   Alvarez|     Sarah|\n",
      "|200-45-4002|Montgomery|    George|\n",
      "|674-90-7121|    Brooks|     Jesse|\n",
      "|093-81-5058|     Davis|   Valerie|\n",
      "|405-71-6928|  Johnston|    Andrea|\n",
      "|285-09-1368| Zimmerman|      Mark|\n",
      "|432-75-9180|     Drake|     Sheri|\n",
      "|287-31-8774|  Campbell| Stephanie|\n",
      "|885-84-0932|    Murphy|    Steven|\n",
      "|299-52-4282|  Andersen|   Lindsay|\n",
      "|572-15-9413|     Clark|   Claudia|\n",
      "|592-79-1854|  Anderson|   Michael|\n",
      "|750-85-4885|    Carter|   Bradley|\n",
      "|795-90-8059|     Lyons|     Chris|\n",
      "|800-68-1097|   Gregory| Stephanie|\n",
      "|528-14-5681|    Garcia|      Anna|\n",
      "+-----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe based on dataDF, so 10,000 records\n",
    "ssnNameDF = dataDF.select(\"ssn\", \"last_name\", \"first_name\")\n",
    "ssnNameDF.show()\n",
    "ssnNameDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|        ssn|          occupation|\n",
      "+-----------+--------------------+\n",
      "|674-90-7121|  Secretary, company|\n",
      "|287-31-8774|Chartered certifi...|\n",
      "|800-68-1097|          Orthoptist|\n",
      "|406-47-0397|Education adminis...|\n",
      "|263-65-2624|Psychologist, pri...|\n",
      "|117-36-4219|                Make|\n",
      "|730-95-2184|    Industrial buyer|\n",
      "|844-93-4799|  Wellsite geologist|\n",
      "|200-54-7515| Animal technologist|\n",
      "|283-17-5670|Biomedical scientist|\n",
      "|547-49-1078|        Bonds trader|\n",
      "|083-24-1465|      Pilot, airline|\n",
      "|138-14-9343|Trading standards...|\n",
      "|688-49-6147|Arts development ...|\n",
      "|415-06-3785|English as a seco...|\n",
      "|381-66-0713|           Ecologist|\n",
      "|553-33-6292|   Financial adviser|\n",
      "|310-62-9455|Chartered legal e...|\n",
      "|170-80-5933|Horticultural the...|\n",
      "|513-51-8267|     Publishing copy|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "957"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe based on sampledDF, so only about 10% of records. This signifies only 10% of the people have a job.\n",
    "ssnOccupationDF = sampledDF.select(\"ssn\", \"occupation\")\n",
    "ssnOccupationDF.show()\n",
    "ssnOccupationDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us preform a join in which we are only interested in the people with an occupation, being an inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------+----------+\n",
      "|        ssn|          occupation|last_name|first_name|\n",
      "+-----------+--------------------+---------+----------+\n",
      "|641-37-0830|Clinical research...|   Wilson| Stephanie|\n",
      "|651-50-0290|Adult guidance wo...|  Trevino|   Anthony|\n",
      "|027-31-3289|Travel agency man...|   Wright|    Amanda|\n",
      "|383-53-3448|Fast food restaur...| Johnston|    Sheila|\n",
      "|741-85-6749|             Barista| Campbell|     Sarah|\n",
      "+-----------+--------------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "957"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinedDF = ssnOccupationDF.join(ssnNameDF, \"ssn\", \"inner\")\n",
    "joinedDF.show(5)\n",
    "joinedDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we perform an outer join, we would get a table of 10,000 rows where occupation is missing for 90% of the people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+----------+\n",
      "|ssn        |occupation|last_name|first_name|\n",
      "+-----------+----------+---------+----------+\n",
      "|062-38-3809|null      |Parks    |Jacob     |\n",
      "|078-99-4683|null      |Johnson  |Daniel    |\n",
      "|091-06-2600|null      |Miller   |Roberto   |\n",
      "|092-19-4852|null      |Rodriguez|Denise    |\n",
      "|123-40-6247|null      |Gonzalez |Ronald    |\n",
      "+-----------+----------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinedDF = ssnOccupationDF.join(ssnNameDF, \"ssn\", \"right\")\n",
    "joinedDF.show(5, truncate=False)\n",
    "joinedDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Caching DataFrames and storage options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8a) Caching DataFrames\n",
    "\n",
    "#### For efficiency Spark keeps your DataFrames in memory. (More formally, it keeps the _RDDs_ that implement your DataFrames in memory.) By keeping the contents in memory, Spark can quickly access the data. However, memory is limited, so if you try to keep too many partitions in memory, Spark will automatically delete partitions from memory to make space for new ones. If you later refer to one of the deleted partitions, Spark will automatically recreate it for you, but that takes time.\n",
    "\n",
    "#### So, if you plan to use a DataFrame more than once, then you should tell Spark to cache it. You can use the `cache()` operation to keep the DataFrame in memory. However, you must still trigger an action on the DataFrame, such as `collect()` or `count()` before the caching will occur. In other words, `cache()` is lazy: It merely tells Spark that the DataFrame should be cached _when the data is materialized_. You have to run an action to materialize the data; the DataFrame will be cached as a side effect. The next time you use the DataFrame, Spark will use the cached data, rather than recomputing the DataFrame from the original data.\n",
    "\n",
    "#### You can see your cached DataFrame in the \"Storage\" section of the Spark web UI. If you click on the name value, you can see more information about where the the DataFrame is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "storageLevel must be of type pyspark.StorageLevel",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-8be04ba08a87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cache the DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfilteredDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"disk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Trigger an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilteredDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Check if it is cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/spark-2.4.0/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mpersist\u001b[0;34m(self, storageLevel)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \"\"\"\n\u001b[1;32m    617\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mjavaStorageLevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getJavaStorageLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorageLevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjavaStorageLevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/spark-2.4.0/python/pyspark/context.py\u001b[0m in \u001b[0;36m_getJavaStorageLevel\u001b[0;34m(self, storageLevel)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \"\"\"\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorageLevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStorageLevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storageLevel must be of type pyspark.StorageLevel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0mnewStorageLevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStorageLevel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: storageLevel must be of type pyspark.StorageLevel"
     ]
    }
   ],
   "source": [
    "# Cache the DataFrame\n",
    "filteredDF.persist()\n",
    "# Trigger an action\n",
    "print(filteredDF.count())\n",
    "# Check if it is cached\n",
    "print(filteredDF.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8b) Unpersist and storage options\n",
    "\n",
    "#### Spark automatically manages the partitions cached in memory. If it has more partitions than available memory, by default, it will evict older partitions to make room for new ones. For efficiency, once you are finished using cached DataFrame, you can optionally tell Spark to stop caching it in memory by using the DataFrame's `unpersist()` method to inform Spark that you no longer need the cached data.\n",
    "\n",
    "#### ** Advanced: ** Spark provides many more options for managing how DataFrames cached. For instance, you can tell Spark to spill cached partitions to disk when it runs out of memory, instead of simply throwing old ones away. You can explore the API for DataFrame's [persist()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.persist) operation using Python's [help()](https://docs.python.org/2/library/functions.html?highlight=help#help) command.  The `persist()` operation, optionally, takes a pySpark [StorageLevel](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.StorageLevel) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# If we are done with the DataFrame we can unpersist it so that its memory can be reclaimed\n",
    "filteredDF.unpersist()\n",
    "# Check if it is cached\n",
    "print(filteredDF.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
